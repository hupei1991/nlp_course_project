#+title: Our Collective Project Report

Write a succinct paragraph for your section!  Your audience is an external
visitor who wants to know what we did for our project.

Sketch a draft for your paragraph, and push it up to github so
that :toni: can make suggestions.


* Goal :meshal:

Spam is annoying! The statistics shows that spam messages make up to 60% of the email traffic worldwide. Fortunately, most of the email service provider includes a spam filter to automatically detect spams and move them in a separate folder, usually called junk. Therefore, the main goal of this project is to implement a spam filter for emails using machine learning classification techniques with the help of NLTK toolkit. NLTK provides wide functionality for natural language processing. Processing the text of each email is really important in order to extract some informative features that can be used to train our classifier. For instance, word occurrence and frequency are some of the most informative features to detect spams. The classifier, Naive Bayes for example, at the end of this project should distinguish between spam and ham (non-spam) based on those features of each email. 

* Corpora :kayla:
We created a corpus of the emails and their respective tags. The emails were in the corpus and they were worked on and uploaded back into the corpora. The corpus kept the emails together so the code could be ran, but also the placed ids on the different emails were kept in the corpus. A dictionary that held our corpus was used to allow us to read the file back in and keep its original structure.

* Approach :chris:
Meshal made a corpus from pre-processed enails, which is divided into two
catogories: spam and ham. Then, Chris  put the entire contents of each email in
a string. James segmented each restringified email into sentences, and
saved them in the appropriate formant. Kayla toeknized each sentence from
each restringified email and saved in the appropriate format. Derek made
all the emails lowercase. Finally, Pei removed the stop words of each
email. We divided the work evenly between us, and Chris got some help from James.
* Implementation :derek:

* Results :pei:

* Analysis and Future Work 					      :james:

Future work should emphasize greater feature generation on the
corpus. 

Future analyses shoud also include a corpus of more recent emails. The
Enron dataset is from the early 2000s, a time in which emailing was more
sparsely used by the general public. As email became more ubiquitous,
spam blockers has become more sophisticated and spammers have developed
more elegant methods for creating spam. This creates a virtual "Red Queen"
scenario. In other words, spam of the early 2000s will be less relevant
than the spam of today given that both spam blockers and spammers have
become more sophisticated.
