#+title: Our Collective Project Report

Write a succinct paragraph for your section!  Your audience is an external
visitor who wants to know what we did for our project.

Sketch a draft for your paragraph, and push it up to github so
that :toni: can make suggestions.


* Goal :meshal:

Spam is annoying! The statistics shows that spam messages make up to 60% of the email traffic worldwide. Fortunately, most of the email service provider includes a spam filter to automatically detect spams and move them in a separate folder, usually called junk. Therefore, the main goal of this project is to implement a spam filter for emails using machine learning classification techniques with the help of NLTK toolkit. NLTK provides wide functionality for natural language processing. Processing the text of each email is really important in order to extract some informative features that can be used to train our classifier. For instance, word occurrence and frequency are some of the most informative features to detect spams. The classifier, Naive Bayes for example, at the end of this project should distinguish between spam and ham (non-spam) based on those features of each email. 

* Corpora :kayla:
We created a corpus of the emails and their respective tags. The emails were in the corpus and they were worked on and uploaded back into the corpora. The corpus kept the emails together so the code could be ran, but also the placed ids on the different emails were kept in the corpus. A dictionary that held our corpus was used to allow us to read the file back in and keep its original structure.

https://github.com/tonikazic/munlp_f16/blob/preliminary_code/code/kayla.org
I tokenized the words of the segmented sentences using nltk's word_tokenize. I first read James' file in, ran the code, and used pickle dump to write the file back in under the tokenized and tokenize.p file.

* Approach :chris:
Meshal made a corpus from pre-processed enails, which is divided into two
catogories: spam and ham. Then, Chris  put the entire contents of each email in
a string. James segmented each restringified email into sentences, and
saved them in the appropriate formant. Kayla toeknized each sentence from
each restringified email and saved in the appropriate format. Derek made
all the emails lowercase. Finally, Pei removed the stop words of each
email. We divided the work evenly between us, and Chris got some help from James.

* Implementation :derek:
[[file:../code/derek.org][Derek's Code]]
I took in a list of tuples as inputs from the tokenized directory.  I iterated over the list with two for loops
and lowered the second element in each list and placed them back in the same structure.
I returned a list of tuples to segmented directory.

* Results :pei:
[[file:../code/pei.org][Pei's File]]
The main target for this project is to get the sequenced words list for
each sentence, which is defined an identifier.
The input would be included in these two corpus directory:
[[file:../data/corpus/ham/][Ham Folder]], and [[file:../data/corpus/spam/][Spam Folder]].
The output would be stored in [[file:../data/remove_stopwords/stopwords_removed.p][this python pickle file]].
The sample output would be like:
#+BEGIN_EXAMPLE
('0001.1999-12-10.farmer.ham.txt::0000', ['Subject', ':', 'christmas', 'tree', 'farm', 'pictures'])
('0001.1999-12-10.kaminski.ham.txt::0000', ['Subject', ':', ':', 'rankings', 'thank', '.'])
('0002.1999-12-13.farmer.ham.txt::0000', ['Subject', ':', 'vastar', 'resources', ',', 'inc.', 'gary', ',', 'production', 'the', 'high', 'island', 'larger', 'block', '-', '1', '#', '2', 'commenced', 'saturday', '2', ':', '00', 'p.', 'm.', 'about', '6', ',', '500', 'gross', '.'])
('0002.1999-12-13.farmer.ham.txt::0001', ['carlos', 'expects', '9', ',', '500', '10', ',', '000', 'gross', 'tomorrow', '.'])
('0002.1999-12-13.farmer.ham.txt::0002', ['vastar', 'owns', '68', '%', 'the', 'gross', 'production', '.'])
('0002.1999-12-13.farmer.ham.txt::0003', ['george', 'x', '3', '-', '6992', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'forwarded', 'george', 'weissman', '/', 'hou', '/', 'ect', '12', '/', '13', '/', '99', '10', ':', '16', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'daren', 'j', 'farmer', '12', '/', '10', '/', '99', '10', ':', '38', 'to', ':', 'carlos', 'j', 'rodriguez', '/', 'hou', '/', 'ect', '@', 'ect', 'cc', ':', 'george', 'weissman', '/', 'hou', '/', 'ect', '@', 'ect', ',', 'melissa', 'graves', '/', 'hou', '/', 'ect', '@', 'ect', 'subject', ':', 'vastar', 'resources', ',', 'inc.', 'carlos', ',', 'please', 'call', 'linda', 'get', 'everything', 'set', '.'])
('0002.1999-12-13.farmer.ham.txt::0004', ["'", 'going', 'estimate', '4', ',', '500', 'coming', 'tomorrow', ',', 'a', '2', ',', '000', 'increase', 'following', 'day', 'based', 'my', 'conversations', 'bill', 'fischer', 'bmar', '.'])
('0002.1999-12-13.farmer.ham.txt::0005', ['d.', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'forwarded', 'daren', 'j', 'farmer', '/', 'hou', '/', 'ect', '12', '/', '10', '/', '99', '10', ':', '34', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'enron', 'north', 'america', 'corp.', ':', 'george', 'weissman', '12', '/', '10', '/', '99', '10', ':', '00', 'to', ':', 'daren', 'j', 'farmer', '/', 'hou', '/', 'ect', '@', 'ect', 'cc', ':', 'gary', 'bryan', '/', 'hou', '/', 'ect', '@', 'ect', ',', 'melissa', 'graves', '/', 'hou', '/', 'ect', '@', 'ect', 'subject', ':', 'vastar', 'resources', ',', 'inc.', 'darren', ',', 'attached', 'appears', 'be', 'nomination', 'vastar', 'resources', ',', 'inc.', 'the', 'high', 'island', 'larger', 'block', '-', '1', '#', '2', '(', 'previously', ',', 'erroneously', 'referred', 'as', '#', '1', 'well', ')', '.'])
('0002.1999-12-13.farmer.ham.txt::0006', ['vastar', 'expects', 'well', 'commence', 'production', 'sometime', 'tomorrow', '.'])
('0002.1999-12-13.farmer.ham.txt::0007', ['told', 'linda', 'harris', 'we', "'", 'get', 'a', 'telephone', 'number', 'gas', 'control', 'she', 'provide', 'notification', 'the', 'turn', '-', 'tomorrow', '.'])
#+END_EXAMPLE
For the first item of each tuple, it's the index identity for sentences in the corpus.
'::' is used to separate the email ID and the sentences' IDs within it. For the second item of each tuple,
it's a list of words, where the stopwords are all removed. In this case, we can use those words to identify whether a piece of email is spam or ham.
* Analysis and Future Work 					      :james:
[[file:../code/james.org][james file]]

#+begin_src python
import pickle
from itertools import chain
from nltk import *

Text(ham_list).collocations()

corpus = pickle.load( open( "../data/remove_stopwords/stopwords_removed.p", "rb" ) )

ham = [v for (k,v) in corpus if "ham" in k]
spam = [v for (k,v) in corpus if "spam" in k]

ham_list = list(chain.from_iterable(ham))
spam_list = list(chain.from_iterable(spam))

ham_dist = FreqDist(ham_list)
spam_dist = FreqDist(spam_list)

# 5000 most frequent words 
ham_dist.most_common(5000)
spam_dist.most_common(5000)

# collocations
ham_coll = Text(ham_list).collocations()
spam_coll = Text(spam_list).collocations()

# bigrams
print(list(trigrams(ham_list)))
print(list(trigrams(spam_list)))
#+end_src

Once the data were pre-processed, we combined the lists of ham and spam
sentences into two lists of ham and spam respectively, where each
list were the entirety of emails (either ham or spam) and each item was a
token in the emails. We then used the FreqDist method from nltk to find the
5000 most common tokens in both the spam and ham emails. These lists were
then transformed into nltk.Text objects in order to find the collocations
of each as well as the bigrams. 

Future analyses should implement a machine learning approach to classifying
emails into either a ham or spam category. This will involve feature
generation (ie. frequency of tokens per category) as input to classifying
emails into their respective categories.

Future analyses shoud also include a corpus of more recent emails. The
Enron dataset is from the early 2000s, a time in which emailing was more
sparsely used by the general public. As email became more ubiquitous,
spam blockers has become more sophisticated and spammers have developed
more elegant methods for creating spam. This creates a virtual "Red Queen"
scenario. In other words, spam of the early 2000s will be less relevant
than the spam of today given that both spam blockers and spammers have
become more sophisticated.


