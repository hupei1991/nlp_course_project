#+title: Sentence Segmentation

* Section 1:  Main Script

We have decided to create a dictionary as our data structure to house our
corpus. From the following script provides us with a dictionary that
contains emails segmented by sentences. The index name, which we structure
as "filename::sentenceInd#" (ex. '1986.2004-08-26.GP.spam.txt::0'
references the first sentence of the 1986.2004-08-26.GP.spam.txt file) is
the key to the dictionary and the sentence is the value. In this script,
this dictionary object is called "fileidxsentenceidx_sentence". However,
accessing you can access this data structures in the data/segmented
directory under the file name sents_dict.p. Instructions for reading this
back in to operate on are in section 3 of this file (james.org).

Instructions for the next step (word segementation) can be found in section
4 of this document.

#+begin_src python :session
# Please put this file in /data/ directory :pei:

import os
import nltk
import random
import re
from nltk import sent_tokenize

# Read the whole folder and return a list of texts :meshal:
# Revised by :pei
def readFolder(folder, hamOrSpam, fileidx):
    for filename in os.listdir(folder):
        fileidx_filename[fileidx[0]] = filename
        f = open(folder + filename, 'r', errors ='ignore') # added ignore for character that couldn't be read :james:
        fileidx_content[fileidx[0]] = f.read()
        fileidx_class[fileidx[0]] = hamOrSpam
        f.close()
        fileidx[0] += 1

# create a method to remove newline characters :chris:
def restringify(email):
    n = email.split('\n')
    s = ' '.join(n)
    return s

# Restringify, and segment the contents of each piece of email into sentences
# Split the emails by sentence :james:

# Also get rid spaces in front of '?' , and '!', according to the definition
# of Punkt Sentence Segmenter;  revised by :pei:
def sentenceSegmenter(email):
    import re
    from nltk import sent_tokenize
    # remove spaces between last word and punctuation. Since punctionation is tokenized
    # each symbol is returned :james:
    symbols_removed = re.sub(r'\s+(?:(\.+)|(\?+)|(!+))', r'\1\2\3', email)
    # remove the spaces before the period
    # from here, we would only consider the '$' sign in order to reduce the feature dimension :pei:
    money_spaces_removed = re.sub(r'(\$)(\s+)(\d+\.)(\s+)(\d+)', r'\1\3\5', symbols_removed) # remove the spaces involved with money :james:
    sents = sent_tokenize(money_spaces_removed)
    return sents


# Use a main function to organize each piece :pei:
# I will not use any other specific data structure to store metadata except for these dictionarys :pei:
if __name__ == "__main__":
    global fileidx_filename, fileidx_class, fileidx_content, fileidx_sentences
    fileidx_filename = {} # {file_name : file_index} :pei:
    fileidx_class = {} # {file_index : file_class} :pei:
    fileidx_content = {} # {file_index : file_content} :pei:
    fileidxsentenceidx_sentence = {} # {file_index::sentence_index : sentence} :pei:

    fileidx = [0] # file_index :pei:
    # Run readFolder separately on ham and spam to get dict data stored :pei:
    readFolder('data/corpus/ham/' , 'ham', fileidx)
    readFolder('data/corpus/spam/' , 'spam', fileidx)
    # Since we are not going to use the fileidx viarable again, we delete it from the memory stack :pei:
    del fileidx
    # We don't actually need to shuffle because they are in a dictionary which is unordered :pei:

    # Generate the dictionary of fileidx_sentences, by applying restringify and sentenseSegmenter on contents :pei:
    # with key of file_index::sentence_index, and segmented sentences as value :pei:
    for fileidx, content in fileidx_content.items():
        for idx, sent in enumerate(sentenceSegmenter(restringify(content))):
            fileidxsentenceidx_sentence['{}::{:04}'.format(fileidx_filename[fileidx],idx)] = sent
#+end_src

* Section 2: Writing the fileidxsentenceidx_sentence dictionary to a file

The following used pickle to write the fileidxsentenceidx_sentence
dictionary to a bytes file using the pickle package. This allows you to
read the file back in and it maintains its original structure (a dictionary).

#+begin_src python :session 
import pickle

pickle.dump(fileidxsentenceidx_sentence, open( "data/segmented/sents_dict.p", "wb" ) )
#+end_src


* Section 3: Reading the dictionary back in

Instead of running the entire script above, you can read the file directly
into memory by using pickle again. In the example below, the dictionary
object is read back in and called "segmented_sents". This object is
identical to the fileidxsentenceisx_sentence dictionary that section 1 gave
you.

#+begin_src python :session
import pickle

segmented_sents = pickle.load( open( "../data/segmented/sents_dict.p", "rb" ) )
#+end_src

We can read this in again and then turn the items into a list of tuples and
then write this back to disk using pickle. This item is now a list of
tuples. 

#+begin_src python :session
sent_list = pickle.dump(sorted(segmented_sents.items()), open("../data/segmented/sent_list.p", "wb"))
#+end_src


Now we can read this list file back in using pickle. This is a list of
tuples and the first item of each tuple is the index and the second item is
the sentence.

#+begin_src python
import pickle

segmented_sents = pickle.load( open( "../data/segmented/sent_list.p", "rb" ) )
#+end_src


* Section 4: Instructions for next step

To tokenize words, you can use the same data structure as the
"fileidxsentenceidx_sentence" dictionary with the same index as key,
sentence as value. However, the sentence will be tokenized by word. For
example:

#+BEGIN_EXAMPLE
# original dictionary would output like so:

{'1986.2004-08-26.GP.spam.txt::0': 'Subject: bloodline , ahead of the street microcap alert when living with sheriff is obsequious , blood clot beyond deficit reach an understanding with toward blood clot.',
 '5394.2001-04-23.kaminski.ham.txt::98': 'the proposal has already struck panic.',...}

# the new dictionary would output:

{'1986.2004-08-26.GP.spam.txt::0': ['Subject:,'bloodline',',','ahead', 'of', 'the', 'street', 'microcap', 'alert', 'when', 'living', 'with', 'sheriff', 'is', 'obsequious', ',', 'blood', 'clot', 'beyond', 'deficit', 'reach','an', 'understanding', 'with', 'toward', 'blood', 'clot','.'],
 '5394.2001-04-23.kaminski.ham.txt::98':['the', 'proposal', 'has', 'already', 'struck', 'panic','.'],...}


#+END_EXAMPLE

In other words, you will just need to iterate over the items of the read in
sents_dict.p file and tokenize each sentence. There is no longer a nead to
create a uniqe index for each word. 
