#+title: Word Tokenizer
 
#+begin_src python :results :kayla 02/14/2016:
import nltk, re, pprint
from nltk import word_tokenize
import pickle

pickle.dump(fileidxsentenceidx_sentence, open( "data/segmented/sents_dict.p", "wb" ) )

def wordTokenizer(sentence):
    from nltk.tokenize import RegexpTokenizer
    toker = RegexpTokenizer(r'\w+|\$+')
 
    return toker.tokenize(sentence)

import pickle

segmented_sents = pickle.load( open( "data/segmented/sents_dict.p", "rb" ) )

#+end_src


